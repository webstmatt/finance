{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd, numpy as np, datetime\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def earnings_yahoo(dateToCheck, printURL = False):\n",
    "    \"\"\"Scape yahoo finance for earnings data for a single date.  It doesnt scrape for a particular symbol.\"\"\"\n",
    "\n",
    "    #yahoo finance has the earnings data on a page which has a range of dates. A range is required for the page to load.\n",
    "    #prepare the dates\n",
    "    startDate =  dateToCheck + dt.timedelta(days=-6)\n",
    "    endDate =  dateToCheck\n",
    "    startDateStr = startDate.strftime(\"%Y-%m-%d\")\n",
    "    endDateStr = endDate.strftime(\"%Y-%m-%d\")\n",
    "    dateToCheckStr = dateToCheck.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    #prepare the URL for scraping.\n",
    "    quote_page = 'https://finance.yahoo.com/calendar/earnings?from='+startDateStr+'&to='+endDateStr+'&day='+dateToCheckStr\n",
    "    if printURL:\n",
    "        print quote_page\n",
    "    \n",
    "    #open the URL using urllib and beautiful soup. parse the page an scrape for the particular table.\n",
    "    page = urllib2.urlopen(quote_page)\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    name_box = soup.find('table', attrs={'class': 'data-table W(100%) Bdcl(c) Pos(r) BdB Bdc($c-fuji-grey-c)'})\n",
    "    if name_box == None:\n",
    "        return None\n",
    "    table_body = name_box.find('tbody')\n",
    "\n",
    "    #generate the dataset using the table if it was found.\n",
    "    data = []\n",
    "    rows = table_body.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        data.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "    \n",
    "    #generate the list of headers for the table.\n",
    "    header = []\n",
    "    cols = name_box.find('thead').find_all('tr')[0].find_all('span')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    header.append([ele for ele in cols if ele])\n",
    "    header = header[0]\n",
    "    \n",
    "    #convert to a dataframe for the data to be returned\n",
    "    returnedData = pd.DataFrame(data, columns=header)\n",
    "    returnedData['date'] = dateToCheck\n",
    "    \n",
    "    return returnedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def earnings_fixTypes(earnings):\n",
    "    '''correct the data types for the earnings dataset'''\n",
    "    earnings['EPS Estimate'] = pd.to_numeric(earnings['EPS Estimate'], errors='coerce')\n",
    "    earnings['Reported EPS'] = pd.to_numeric(earnings['Reported EPS'], errors='coerce')\n",
    "    earnings['Surprise(%)'] = pd.to_numeric(earnings['Surprise(%)'], errors='coerce')\n",
    "    earnings['Symbol'] = earnings['Symbol'].astype(str)\n",
    "    return earnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def earnings_yahoo_range(start, end, printURL = False, fixDataTypes=True):\n",
    "    \"\"\"Scape yahoo finance for earnings data between two dates. It doesnt scrape for a particular symbol.\"\"\"\n",
    "\n",
    "    #prepare dates\n",
    "    startDate = dt.datetime.strptime(start, '%Y-%m-%d')\n",
    "    endDate = dt.datetime.strptime(end, '%Y-%m-%d')\n",
    "\n",
    "    earningsData = pd.DataFrame()\n",
    "    \n",
    "    #loop through each day in the date range and call earnings_yahoo to compile the data.\n",
    "    totalDays = (endDate-startDate).days+1\n",
    "    for i in range(0,totalDays):\n",
    "        dateToRetrieve = startDate + dt.timedelta(days=i)\n",
    "        helpers.progress('Processing earnings: ' + str(dateToRetrieve) + ' | Day: ' + str(i+1) + ' of ' + str(totalDays))\n",
    "        retrievedDF = earnings_yahoo(dateToRetrieve, printURL)\n",
    "        if retrievedDF is not None:\n",
    "            earningsData = earningsData.append(retrievedDF, printURL)\n",
    "    \n",
    "    earningsData['announce'] = \"\"\n",
    "    earningsData.loc[earningsData['Earnings Call Time'] == \"Before Market Open\",'announce'] = \"0:00\"\n",
    "    earningsData.loc[earningsData['Earnings Call Time'] == \"After Market Close\",'announce'] = \"23:59\"\n",
    "\n",
    "    #remove the items that didnt match \"Before Market Open / Close\" -- note this could be fixed later by parsing the date str\n",
    "    #for example, if it says \"12PM\" then we are ignoring the data\n",
    "    earningsData = earningsData[earningsData['announce'] <> \"\"] \n",
    "\n",
    "    #convert the announcement date and the time to a \"dt\" value representing the time of the announcement. \n",
    "    #We assume an overnight announcement is at midnight. \n",
    "    earningsData['dtString'] = earningsData['date'].astype(str) + ' ' + earningsData['announce'].astype(str)\n",
    "    earningsData['dt'] = pd.to_datetime(earningsData['dtString'], format='%Y-%m-%d %H:%M')\n",
    "    \n",
    "    #Drop the date and annouce columns. And dtString isnt required anymore\n",
    "    earningsData = earningsData.drop(['dtString'],axis=1)\n",
    "    earningsData = earningsData.drop(['date'],axis=1)\n",
    "    earningsData = earningsData.drop(['announce'],axis=1)\n",
    "    earningsData = earningsData.drop(['Earnings Call Time'],axis=1)\n",
    "    earningsData = earningsData.drop(['Company'],axis=1)\n",
    "    \n",
    "    if fixDataTypes:\n",
    "        earningsData = earnings_fixTypes(earningsData)\n",
    "    return earningsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prices_google(intervalMins=30, periodDays=90, symbol='AAPL', printURL=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prices_google(intervalMins, periodDays, symbol, intervalMins=30, periodDays=90, printURL = False):\n",
    "    \"\"\"Scape google finance for price data for one symbol. interval is in minutes, period is number of days.\"\"\"\n",
    "    \"\"\"Example URL: https://www.google.com/finance/getprices?q=AAPL&i=86400&p=14d&f=d,o,h,l,c,v\"\"\"\n",
    "    \n",
    "    #prepare parameters\n",
    "    interval = intervalMins*60\n",
    "    period = str(periodDays)+'d'\n",
    "    url = \"http://www.google.com/finance/getprices?q=\"+symbol+\"&i=\"+str(interval)+\"&p=\"+period+\"&f=d,o,h,l,c,v\"\n",
    "    \n",
    "    #print the URL if requested\n",
    "    if printURL:\n",
    "        print url\n",
    "    \n",
    "    #scrape data using read_csv\n",
    "    data=list(np.array(pd.read_csv(url,skiprows=7,header=None)))\n",
    "    \n",
    "    #declare arrays - date is for capturing timestamps, proc is for the processed data.\n",
    "    date = []\n",
    "    proc = []\n",
    "    \n",
    "    #loop through the scraped data and append to date and proc arrays.\n",
    "    #some processing is required because of the google format. it has offsets that must be added.\n",
    "    for i in range(0,len(data)):\n",
    "        if data[i][0][0]=='a':\n",
    "            t= datetime.datetime.fromtimestamp(int(data[i][0].replace('a','')))\n",
    "            date.append(t)\n",
    "            proc.append(data[i])\n",
    "        elif data[i][0][0]=='T':\n",
    "            continue\n",
    "        else:\n",
    "            date.append(t+datetime.timedelta(minutes=int(data[i][0])*interval/60))\n",
    "            proc.append(data[i])\n",
    "    \n",
    "    #prepare dataframe for returning from function.\n",
    "    final=pd.DataFrame(proc,index=date)\n",
    "    final.columns=['a','Open','High','Low','Close','Vol']\n",
    "    final['Symbol'] = symbol\n",
    "    final = final.drop(columns=[\"a\"])\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prices_google_symbols(symbols, intervalMins=30, periodDays=90):\n",
    "    '''Scrape google for price data with a list of symbols.'''\n",
    "    prices = pd.DataFrame()\n",
    "    \n",
    "    #i and total are for returning the progress statement\n",
    "    i, total = 0, len(symbols)\n",
    "    \n",
    "    #loop through the list of symbols (dataframe) and call the prices_google function to scrape the data.\n",
    "    for index, row in symbols.iterrows():\n",
    "        i+=1\n",
    "        symbol = row['Symbol']\n",
    "        helpers.progress('Processing prices: ' + symbol + ' | Progress: ' + str(i) + ' of ' + str(total))\n",
    "        symbol_price = prices_google(intervalMins=intervalMins, periodDays=periodDays, symbol=symbol)\n",
    "        prices = prices.append(symbol_price)\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stocksummary_yahoo(symbol):\n",
    "    '''Scrape yahoo finance for the stock data summary such as PEratio, Avg Volume, Market Cap for a single symbol.'''\n",
    "    \n",
    "    #prepare URL\n",
    "    quote_page = 'https://finance.yahoo.com/quote/'+symbol+'/'\n",
    "    \n",
    "    #setup the web scraper\n",
    "    page = urllib2.urlopen(quote_page)\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    #find the quote-summary div to return the table\n",
    "    name_box = soup.find('div', attrs={'id': 'quote-summary'})\n",
    "    table_body= name_box\n",
    "    data = []\n",
    "    rows = table_body.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        data.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "    \n",
    "    x = pd.DataFrame(data)\n",
    "    x = x.transpose()\n",
    "    x.columns = x.iloc[0]\n",
    "    x = x.reindex(x.index.drop(0))\n",
    "    x.insert(0,\"Symbol\", value=symbol)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stocksummary_fixTypes(stocksummary):\n",
    "    '''correct the data types and columns for the stock summary dataset'''\n",
    "    \n",
    "    #convert and split text for applicable columns\n",
    "    stocksummary['Forward Dividend'] = stocksummary['Forward Dividend & Yield'].apply(lambda x: x.split(' (')[0])\n",
    "    stocksummary['Forward Yield'] = stocksummary['Forward Dividend & Yield'].apply(lambda x: x.split(' (')[1][:-2])\n",
    "    stocksummary['Bid Price'] = stocksummary['Bid'].apply(lambda x: x.split(' x ')[0])\n",
    "    stocksummary['Bid Lot'] = stocksummary['Bid'].apply(lambda x: x.split(' x ')[1])\n",
    "    stocksummary['Ask Price'] = stocksummary['Ask'].apply(lambda x: x.split(' x ')[0])\n",
    "    stocksummary['Ask Lot'] = stocksummary['Ask'].apply(lambda x: x.split(' x ')[1])\n",
    "    stocksummary['Day Range Min'] = stocksummary[\"Day's Range\"].apply(lambda x: x.split(' - ')[0])\n",
    "    stocksummary['Day Range Max'] = stocksummary[\"Day's Range\"].apply(lambda x: x.split(' - ')[1])\n",
    "    stocksummary['52 Week Range Min'] = stocksummary['52 Week Range'].apply(lambda x: x.split(' - ')[0])\n",
    "    stocksummary['52 Week Range Max'] = stocksummary['52 Week Range'].apply(lambda x: x.split(' - ')[1])\n",
    "    \n",
    "    #convert columns to floats if they are actually numerical.\n",
    "    columnsToFloat = ['Previous Close', 'Open', 'Volume', 'Avg. Volume', 'Market Cap', 'Beta',\n",
    "       'PE Ratio (TTM)', 'EPS (TTM)', '1y Target Est','Forward Dividend','Forward Yield',\n",
    "       'Bid Price', 'Bid Lot', 'Ask Price', 'Ask Lot','Day Range Min','Day Range Max',\n",
    "        '52 Week Range Min','52 Week Range Max']\n",
    "    stocksummary[columnsToFloat] = stocksummary[columnsToFloat].applymap(lambda x: text_to_num(x))\n",
    "\n",
    "    #fix fields that should be in percentage\n",
    "    stocksummary['Forward Yield'] = stocksummary['Forward Yield']/100\n",
    "    \n",
    "    return stocksummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stocksummary_yahoo_symbols(symbols, fixDataTypes=True):\n",
    "    '''Scrape yahoo for stock summary with a list of symbols.'''\n",
    "    \n",
    "    #declare variables to hold data for function.\n",
    "    stocksummary = pd.DataFrame()\n",
    "    success = []\n",
    "    fail = []\n",
    "    \n",
    "    #i and total are for returning the progress statement\n",
    "    i, total = 0, len(symbols)\n",
    "    \n",
    "    #loop through list of symbols and call the function to scrape yahoo.\n",
    "    for index, row in symbols.iterrows():\n",
    "        i+=1\n",
    "        symbol = row['Symbol']\n",
    "        helpers.progress('Processing stock summary: ' + str(symbol) + ' | Progress: ' + str(i) + ' of ' + str(len(symbols)))\n",
    "        try:\n",
    "            scrape = stocksummary_yahoo(symbol=symbol)\n",
    "            stocksummary = stocksummary.append(scrape) \n",
    "            success.append(symbol)\n",
    "        except:\n",
    "            fail.append(symbol)\n",
    "\n",
    "    if fixDataTypes:\n",
    "        stocksummary = stocksummary_fixTypes(stocksummary)\n",
    "    \n",
    "    print '\\nsuccess: ' + str(len(success)) + ' | ' + 'fail: ' + str(len(fail))\n",
    "    \n",
    "    return stocksummary, success, fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
